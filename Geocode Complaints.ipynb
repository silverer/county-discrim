{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "import openpyxl\n",
    "import regex as re\n",
    "import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_path = \"../../Library/CloudStorage/Box-Box/EEOC data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset linking states, counties, and FIPS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the DP05_0058E part of the query is just a placeholder--we're doing this so we have a county name-county fip crosswalk\n",
    "q = f\"https://api.census.gov/data/2014/acs/acs5/profile?get=GEO_ID,NAME,DP05_0058E&for=county:*&in=state:*&key={tokens.CENSUS_KEY}\"\n",
    "pop_r = requests.get(q)\n",
    "pop_resp = pop_r.json()\n",
    "headers = pop_resp.pop(0)\n",
    "county_fips_data = pd.DataFrame(pop_resp, columns=headers)\n",
    "county_fips_data = county_fips_data.rename(columns = {\"GEO_ID\":\"geo_id\",\n",
    "                                  \"NAME\": \"county_state\",\n",
    "                                  \"DP05_0058E\": \"total_pop\",\n",
    "                                  \"state\": \"state_fips\",\n",
    "                                  \"county\": \"county_fips\"})\n",
    "county_fips_data[\"CountyFIPS\"] = county_fips_data[\"state_fips\"]+county_fips_data[\"county_fips\"]\n",
    "\n",
    "county_fips_data = county_fips_data[[\"geo_id\",\"CountyFIPS\", \"county_state\", \"state_fips\", \"county_fips\"]]\n",
    "\n",
    "def get_county_name(county_state):\n",
    "    cty_str = county_state.split(\",\")[0]\n",
    "    cty_str = cty_str.replace(\" County\", \"\")\n",
    "    return cty_str\n",
    "def get_state_name(county_state):\n",
    "    state_str = county_state.split(\",\")[1]\n",
    "    state_str = state_str.strip()\n",
    "    return state_str\n",
    "\n",
    "county_fips_data[\"CountyName\"] = county_fips_data[\"county_state\"].apply(get_county_name)\n",
    "county_fips_data[\"StateName\"] = county_fips_data[\"county_state\"].apply(get_state_name)\n",
    "county_fips_data[\"StateAbbr\"] = county_fips_data.StateName.map(utils.us_state_abbrev)\n",
    "county_fips_data[\"CountyName_low\"] = county_fips_data[\"CountyName\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the dataset with latitudes and longitudes for US zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_zip(old_zip):\n",
    "    if pd.isnull(old_zip):\n",
    "        return\n",
    "    if type(old_zip) != str:\n",
    "        old_zip = str(int(old_zip))\n",
    "    if any(s.isalpha() for s in old_zip):\n",
    "        return\n",
    "    if \"-\" in old_zip:\n",
    "        old_zip = old_zip[0:re.search(\"[-]\", old_zip).span()[0]]\n",
    "    \n",
    "    return old_zip.zfill(5)\n",
    "\n",
    "#https://www.unitedstateszipcodes.org/zip-code-database/\n",
    "all_zips = pd.read_csv(box_path+\"zip_code_database.csv\")\n",
    "\n",
    "all_zips[\"fixed_zip\"] = all_zips[\"zip\"].apply(fix_zip)\n",
    "zips = all_zips[all_zips[\"state\"].isin(list(utils.us_state_abbrev.values()))]\n",
    "zips[\"county\"] = zips[\"county\"].str.replace(\" County\", \"\")\n",
    "zips[\"county_low\"] = zips[\"county\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the dataset with unique zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places = pd.read_excel(box_path+\"places_to_geocode.xlsx\", \n",
    "                              engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Clean the locations to prepare them for geocoding\n",
    "\n",
    "This involves salvaging any zipcodes that were incorrectly entered as the city, removing cases that are not within the 50 U.S. states or Washington, D.C., and fixing city names that were originally misspelled and identified via manual review. The city names that need to be fixed manually will become apparent in **Step 3**. You can add the manual fix cities and city-state combinations to a reference spreadsheet (I've named mine `manual_fix_citystates_v1.xlsx`).\n",
    "\n",
    "The columns include:\n",
    "\n",
    "- For fixing city and state combinations:\n",
    "    - `city_state`: The original city and state combination in the dataset that needs to recoded, e.g., `hoston, texas`\n",
    "    - `city_state_clean`: The fixed city and state combination e.g., `houston, texas`\n",
    "    \n",
    "- For fixing misspelled cities:\n",
    "    - `original_city`: The original city that needs to be recoded, e.g., `detoit`\n",
    "    - `new_city`: The fixed city name e.g., `detroit`\n",
    "    \n",
    "Because the city-state combinations and standalone cities are resolved in separate steps, the location in the `city_state` columns need not match the city listed in the same row's `city` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in spreadsheets to fix spelling errors on cities and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_and_states = pd.read_excel(box_path + \"manual_fix_citystates_v1.xlsx\", engine = \"openpyxl\")\n",
    "tmp = cities_and_states.dropna(subset = [\"original_city\"])\n",
    "recode_cities = dict(zip(tmp[\"original_city\"].to_list(), \n",
    "                        tmp[\"new_city\"].to_list()))\n",
    "tmp = cities_and_states.dropna(subset = [\"city_state\"])\n",
    "tmp[\"city_state\"] = tmp[\"city_state\"].str.lower()\n",
    "tmp[\"city_state_clean\"] = tmp[\"city_state_clean\"].str.lower()\n",
    "\n",
    "recode_city_states = dict(zip(tmp[\"city_state\"].to_list(),\n",
    "                             tmp[\"city_state_clean\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvage_zipcodes(city_state, old_zip):\n",
    "    if pd.isnull(old_zip) and pd.isnull(city_state)==False:\n",
    "        if re.search(\"\\d{5}[,] [A-Z][a-z]\", city_state):\n",
    "            if not re.search(\"\\d{6}[,] [A-Z][a-z]\", city_state):\n",
    "                return re.search(\"\\d{5}\", city_state).group()\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "    elif pd.isnull(old_zip)==False:\n",
    "        if re.search(\"\\d{5}\", old_zip):\n",
    "            if not re.search(\"\\d{6}\", old_zip):\n",
    "                return re.search(\"\\d{5}\", old_zip).group()\n",
    "            else:\n",
    "                return\n",
    "    return old_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"respondent_zip_clean\"] = unique_places.apply(lambda x: salvage_zipcodes(x[\"city_state\"],\n",
    "                                                                                   x[\"respondent_zip_nn\"]),\n",
    "                                                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_state(state):\n",
    "    if pd.isnull(state)==False:\n",
    "        if state == \"Puerto Rico\" or \"FPO\" in state or \"APO\" in state or \"Guam\" in state or \"US Virgin Islands\" in state or \"Marianna\" in state:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "def score_city(city):\n",
    "    if pd.isnull(city) == False:\n",
    "        if re.search(\"[(]\\d+\\d+\", city):\n",
    "            return 0\n",
    "        elif re.search(\"\\d+\", city[0:1]):\n",
    "            return 0\n",
    "        elif len(city) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "def score_county(county):\n",
    "    if pd.isnull(county)==False:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def score_zip(zipcode):\n",
    "    if pd.isnull(zipcode)==False:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def get_loc_score(city_score, state_score, zip_score, county_score):\n",
    "    if state_score ==1:\n",
    "        if city_score == 1 or county_score == 1 or zip_score == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if state_score <0:\n",
    "            return 0\n",
    "        if zip_score ==1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Score\" the locations based on whether it has enough information to get a lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"city_score\"] = unique_places[\"respondent_city_nn\"].apply(score_city)\n",
    "unique_places[\"zip_score\"] = unique_places[\"respondent_zip_clean\"].apply(score_zip)\n",
    "unique_places[\"county_score\"] = unique_places[\"respondent_county_nn\"].apply(score_county)\n",
    "unique_places[\"state_score\"] = unique_places[\"respondent_state_nn\"].apply(score_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"loc_score\"] = unique_places.apply(lambda x: get_loc_score(x[\"city_score\"],\n",
    "                                                                  x[\"state_score\"],\n",
    "                                                                  x[\"zip_score\"],\n",
    "                                                                  x[\"county_score\"]),\n",
    "                                          axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbrevs = dict(zip(list(utils.us_state_abbrev.values()),\n",
    "                         list(utils.us_state_abbrev.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any locations outside the US or in an APO/FPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places.loc_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places = unique_places[unique_places.loc_score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_places[\"respondent_state_abbrev_nn\"] = unique_places[\"respondent_state_nn\"].map(utils.us_state_abbrev)\n",
    "unique_places[\"state_in_us\"] = unique_places[\"respondent_state_abbrev_nn\"].isnull()==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places.loc[pd.isnull(unique_places[\"respondent_county_nn\"])==False, \"state_in_us\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any locations that aren't within the 50 US states or Washington D.C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\"Guam\", \"US Virgin Islands\", \"Palau\", \"Northern Marianna Islands\", \"Puerto Rico\",\n",
    "         \"American Samoa\"]\n",
    "unique_places.loc[unique_places[\"respondent_state_nn\"].astype(str).isin(remove), \"state_in_us\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"state_in_us\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the non-US states from the \"state\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"respondent_state_fixed\"] = unique_places[\"respondent_state_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places.loc[unique_places[\"respondent_state_nn\"].astype(str)==\"District Of Columbia\", \"state_in_us\"] = True\n",
    "unique_places.loc[unique_places[\"respondent_state_nn\"].astype(str)==\"District of Columbia\", \"state_in_us\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"state_in_us\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places.loc[unique_places[\"state_in_us\"]==False, \"respondent_state_fixed\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places.loc[unique_places[\"respondent_state_fixed\"].astype(str).isin(remove), \"respondent_state_fixed\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"zip_state\"] = unique_places[\"respondent_zip_clean\"].astype(str) +\", \"+ unique_places[\"respondent_state_fixed\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function to handle incorrectly-spelled locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_fip(x):\n",
    "    if pd.isnull(x):\n",
    "        return \"\"\n",
    "    else:\n",
    "        x = str(int(float(x)))\n",
    "        return x.zfill(5)\n",
    "#city_recode_dict maps misspelled cities onto their current spelling\n",
    "#the default is the dictionary called recode_cities\n",
    "def fix_city(x, city_recode_dict = recode_cities):\n",
    "    replace_strings = [\"St[.] \", \"St \", \"Ft[.] \", \"Ft \", \"Mt[.] \", \"Mt \"]\n",
    "    new_strings = [\"Saint \", \"Saint \", \"Fort \",\"Fort \", \"Mount \", \"Mount \"]\n",
    "    replace_list = \"|\".join(replace_strings)\n",
    "    replace_list = \"(\"+replace_list+\")\"\n",
    "    if \"Hieghts\" in x:\n",
    "        x = x.replace(\"Hieghts\", \"Heights\")\n",
    "    if pd.isnull(x):\n",
    "        return\n",
    "    else:\n",
    "        x = x.rstrip()\n",
    "        x = x.lstrip()\n",
    "        x = re.sub(\"\\s{2,6}\", \" \", x)\n",
    "        if re.search(replace_list, x):\n",
    "            for i in range(0, len(new_strings)):\n",
    "                if re.search(replace_strings[i], x):\n",
    "                    x = re.sub(replace_strings[i], new_strings[i], x)\n",
    "                    #return x\n",
    "        if x in city_recode_dict.keys():\n",
    "            x = city_recode_dict[x]\n",
    "        x = re.sub(\",\", \"\", x)\n",
    "        return x\n",
    "\n",
    "#city_state_dict maps misspelled cities and states onto their current spelling\n",
    "#the default is the dictionary called recode_city_states\n",
    "def fix_city_state(x, city_state_dict = recode_city_states):\n",
    "    if pd.isnull(x):\n",
    "        return\n",
    "    x = re.sub(\"\\s{2,6}\", \" \", x)\n",
    "    if x in city_state_dict.keys():\n",
    "        x = city_state_dict[x]\n",
    "    return x\n",
    "\n",
    "#city_recode_dict and city_state_dict fix commonly misspelled places. \n",
    "#they default to recode_cities and recode_city_states, respectively\n",
    "def fix_city_and_state(city, state, \n",
    "                       return_type = \"city_state\",\n",
    "                      city_recode_dict = recode_cities,\n",
    "                      city_state_dict = recode_city_states):\n",
    "    if pd.isnull(city) or pd.isnull(state):\n",
    "        return None, None\n",
    "    city = fix_city(city)\n",
    "    if city in city_recode_dict.keys():\n",
    "        city = city_recode_dict[city]\n",
    "    city_state = city + \", \" + state\n",
    "    city_state = city_state.lower()\n",
    "    if city_state in city_state_dict.keys():\n",
    "        city_state = city_state_dict[city_state]\n",
    "        city = city_state.split(\",\")[0]\n",
    "        city = city.lstrip()\n",
    "        city = city.rstrip()\n",
    "    if (\"district of columbia\" in state.lower() or \n",
    "        \"district of columbia\" in city.lower()):\n",
    "        city = \"Washington\"\n",
    "        city_state = \"washington, district of columbia\"\n",
    "        \n",
    "    return city, city_state\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"city\"], unique_places[\"city_state_clean\"] = zip(*unique_places.apply(lambda x: fix_city_and_state(x[\"respondent_city_nn\"],\n",
    "                                                                             x[\"respondent_state_fixed\"]),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places = unique_places[unique_places[\"state_in_us\"]==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Merge the places dataset with the county FIP dataset based on respondent county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[\"respondent_county_nn_low\"] = unique_places[\"respondent_county_nn\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_places_merge = pd.merge(unique_places, \n",
    "                         county_fips_data[[\"StateName\", \"CountyFIPS\",\"CountyName_low\"]], \n",
    "                         left_on = [\"respondent_county_nn_low\", \"respondent_state_fixed\"],\n",
    "                         right_on = [\"CountyName_low\", \"StateName\"], \n",
    "                         how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places_merge = unique_places_merge.drop(columns = [\"StateName\", \"CountyName_low\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places_merge = unique_places_merge.rename(columns = {\"CountyFIPS\":\"state_county_fips\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Merge the places dataset with the lat/long dataset based on respondent zip code\n",
    "\n",
    "Here, exclude any places with a missing zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an identifier for the places\n",
    "unique_places_merge = unique_places_merge.reset_index(drop = True)\n",
    "unique_places_merge[\"place_id\"] = unique_places_merge.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_places_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places_merge.respondent_state_abbrev_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge = pd.merge(unique_places_merge, zips, how = \"left\", \n",
    "                      left_on = [\"respondent_zip_clean\",\"respondent_state_abbrev_nn\"], \n",
    "                     right_on = [\"fixed_zip\", \"state\"])\n",
    "zip_merge['latitude'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the locations that successfully merged on county name and zip code from those that did not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge[\"success\"] = pd.isnull(zip_merge[\"latitude\"])==False\n",
    "zip_merge.loc[pd.isnull(zip_merge[\"state_county_fips\"])==False, \"success\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge[\"county\"] = zip_merge[\"county\"].str.replace(\" County\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge[\"county_low\"] = zip_merge[\"county\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip_merge.state_county_fips.isnull().sum())\n",
    "zip_merge = pd.merge(zip_merge, county_fips_data[[\"CountyName_low\", \"StateAbbr\", \"CountyFIPS\"]],\n",
    "                    left_on = [\"county_low\", \"state\"],\n",
    "                    right_on = [\"CountyName_low\", \"StateAbbr\"],\n",
    "                    how=\"left\")\n",
    "print(zip_merge.CountyFIPS.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge.loc[pd.isnull(zip_merge.state_county_fips), \"state_county_fips\"] = zip_merge[\"CountyFIPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge[\"state_county_fips\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge= zip_merge.drop(columns = [\"CountyName_low\", \"StateAbbr\",\"CountyFIPS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zips[\"state_full\"] = zips[\"state\"].map(state_abbrevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_crossref = zips.copy()\n",
    "zips_crossref[\"city_state_clean\"] = zips_crossref[\"primary_city\"]+\", \"+zips_crossref[\"state_full\"]\n",
    "zips_crossref[\"city_state_lower\"] = zips_crossref['city_state_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Merge places dataset with zipcode dataset and county FIP dataset based on city and state\n",
    "\n",
    "For places that failed to merge on zip codes alone, try to match them to city strings. This section first estimates the correct county name for each failed location based on the city and state name, and then locates the county FIP code using the estimated county name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_df = pd.DataFrame() #track the places that still fail based on the city + state search\n",
    "tracker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_merge[\"city\"] = zip_merge[\"city\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge = zip_merge.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_acceptable(primary, others):\n",
    "    if pd.isnull(others):\n",
    "        return primary\n",
    "    else:\n",
    "        return primary + ', ' + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_crossref[\"all_acceptable\"] = zips_crossref.apply(lambda x: get_all_acceptable(x[\"primary_city\"],\n",
    "                                                                                  x[\"acceptable_cities\"]),\n",
    "                                                     axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_locs = [\"north \", \"east \", \"west \", \"south \",\n",
    "                         \"n. \", \"e. \", \"w. \", \"s. \"]\n",
    "for i in zip_merge.index:\n",
    "    if pd.isnull(zip_merge.loc[i, \"state_county_fips\"])==False:\n",
    "        continue\n",
    "    #try to find a lat/long match for the city\n",
    "    if pd.isnull(zip_merge.loc[i, \"state_county_fips\"]) and (pd.isnull(zip_merge.loc[i, \"city_state_clean\"])==False):\n",
    "        look = zip_merge.loc[i, \"city_state_clean\"].lower()\n",
    "        look = look.lstrip()\n",
    "        look = look.rstrip()\n",
    "        if look in zips_crossref[\"city_state_lower\"].to_list():\n",
    "            tmp = zips_crossref.loc[zips_crossref[\"city_state_lower\"].str.contains(look),:]\n",
    "            tmp = tmp.reset_index(drop= True)\n",
    "            if len(tmp) > 0:\n",
    "                #print(\"FOUND: \", zip_merge.loc[i, \"city_state_clean\"])\n",
    "                for c in zips_crossref.columns:\n",
    "                    if c != \"city_state_lower\" and c != \"city_state_clean\":\n",
    "                        zip_merge.loc[i, c] = tmp.loc[0, c]\n",
    "            else:\n",
    "                tmp = zips_crossref.loc[zips_crossref[\"city_state_lower\"]==look,:]\n",
    "                tmp = tmp.reset_index(drop= True)\n",
    "                if len(tmp) > 0:\n",
    "                    #print(\"FOUND: \", zip_merge.loc[i, \"city_state_clean\"])\n",
    "                    for c in zips_crossref.columns:\n",
    "                        if c != \"city_state_lower\" and c != \"city_state_clean\":\n",
    "                            zip_merge.loc[i, c] = tmp.loc[0, c]\n",
    "        elif any(zip_merge.loc[i, \"city\"].startswith(l) for l in check_locs):\n",
    "            for l in check_locs:\n",
    "                if zip_merge.loc[i, \"city\"].startswith(l):\n",
    "                    \n",
    "                    look = zip_merge.loc[i, \"city_state_clean\"].replace(l, \"\")\n",
    "                    look = look.lower()\n",
    "                    if look in zips_crossref[\"city_state_lower\"].to_list():\n",
    "                        tmp = zips_crossref.loc[zips_crossref[\"city_state_lower\"].str.contains(look),:]\n",
    "                        tmp = tmp.reset_index(drop= True)\n",
    "                        if len(tmp) > 0:\n",
    "                            #print(\"FOUND: \", zip_merge.loc[i, \"city_state_clean\"])\n",
    "                            for c in zips_crossref.columns:\n",
    "                                if c != \"city_state_lower\" and c != \"city_state_clean\":\n",
    "                                    zip_merge.loc[i, c] = tmp.loc[0, c]\n",
    "                            break\n",
    "        else:\n",
    "            look = zip_merge.loc[i, \"city\"].lower().lstrip().rstrip()\n",
    "            tmp = zips_crossref.loc[zips_crossref[\"state\"] ==zip_merge.loc[i, \"respondent_state_abbrev_nn\"],:]\n",
    "            tmp[\"acceptable_city\"] = tmp[\"all_acceptable\"].apply(lambda x: look in x.lower() if type(x)==str else False)\n",
    "            tmp = tmp.loc[tmp[\"acceptable_city\"]==True,:]\n",
    "            tmp = tmp.reset_index(drop = True)\n",
    "            if len(tmp) > 0:\n",
    "                #print(\"FOUND: \", zip_merge.loc[i, \"city_state_clean\"])\n",
    "                for c in zips_crossref.columns:\n",
    "                    if c != \"city_state_lower\" and c != \"city_state_clean\":\n",
    "                        zip_merge.loc[i, c] = tmp.loc[0, c]\n",
    "                        \n",
    "            if pd.isnull(zip_merge.loc[i, \"latitude\"]):\n",
    "                for c in zip_merge.columns:\n",
    "                    fail_df.loc[tracker, c] = zip_merge.loc[i, c]\n",
    "                tracker += 1\n",
    "                \n",
    "                print(\"FAILED: \", zip_merge.loc[i, \"city_state\"], \n",
    "                      \"CLEAN NAME: \", zip_merge.loc[i, \"city_state_clean\"],\n",
    "                      \" COUNTY: \", zip_merge.loc[i, \"respondent_county_nn\"])\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found any places that might have a misspecified state, we'll try merging again based on zip code and city name or county name (whichever is available), this time ignoring the state. This is not as error-prone now that any positive matches based on the searches we've done so far are probably due to a misspecified city name or state name. \n",
    "\n",
    "We'll print out the corresponding primary cities and alternative cities for the zip code to see if the places names might match despite having a different state (indicating that the state was misspecified in error since both the zip code and the place name correspond). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_zips = zip_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_crossref[\"acceptable_w_county\"] = zips_crossref[\"all_acceptable\"] + \", \" + zips_crossref[\"county\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for cases where the state might have been incorrectly specified\n",
    "for i in zip_merge_zips.index:\n",
    "    if pd.isnull(zip_merge_zips.loc[i, \"state_county_fips\"])==False:\n",
    "        continue\n",
    "    if pd.isnull(zip_merge_zips.loc[i, \"latitude\"])==False:\n",
    "        continue\n",
    "    if pd.isnull(zip_merge_zips.loc[i, \"respondent_zip_clean\"])==False:\n",
    "        tmp = zips_crossref.loc[zips_crossref[\"fixed_zip\"]==zip_merge_zips.loc[i, \"respondent_zip_clean\"],:]\n",
    "        tmp = tmp.reset_index(drop = True)\n",
    "        if pd.isnull(zip_merge_zips.loc[i, \"city\"])==False:\n",
    "            look = zip_merge_zips.loc[i, \"city\"].lower()\n",
    "        elif pd.isnull(zip_merge_zips.loc[i, \"respondent_county_nn\"])==False:\n",
    "            look = zip_merge_zips.loc[i, \"respondent_county_nn\"].lower()\n",
    "        elif pd.isnull(zip_merge_zips.loc[i, \"respondent_city_nn\"])==False:\n",
    "            look = zip_merge_zips.loc[i, \"respondent_city_nn\"].lower()\n",
    "        else:\n",
    "            print(\"nothing to search for: \", zip_merge_zips.loc[i, \"respondent_county_nn\"],\n",
    "                 \" \", zip_merge_zips.loc[i, \"city\"])\n",
    "            continue\n",
    "        if len(tmp) > 0:\n",
    "            tmp[\"acceptable_city\"] = tmp[\"acceptable_w_county\"].apply(lambda x: \n",
    "                                                                      look in x.lower() if type(x)==str else False)\n",
    "            \n",
    "            if tmp.loc[0, \"acceptable_city\"]==True:\n",
    "                print(\" -------\")\n",
    "                print(\"FOUND: \", zip_merge_zips.loc[i, \"city_state_clean\"])\n",
    "                print(\"Acceptable cities: \", tmp.loc[0, \"acceptable_cities\"])\n",
    "                print(\"Primary cities: \", tmp.loc[0, \"primary_city\"])\n",
    "                print(\"County: \", tmp.loc[0, \"county\"])\n",
    "                print(\"State abb.: \", tmp.loc[0, \"state\"])\n",
    "                print(\" -------\")\n",
    "                print(\"  \\n\")\n",
    "                for c in zips_crossref.columns:\n",
    "                    if c != \"city_state_lower\" and c != \"city_state_clean\":\n",
    "                        zip_merge_zips.loc[i, c] = tmp.loc[0, c]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the fixed county name information from the city-based search to get the FIP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip_merge_zips.state_county_fips.isnull().sum())\n",
    "zip_merge_zips[\"county\"] = zip_merge_zips[\"county\"].str.replace(\" County\", \"\")\n",
    "zip_merge_zips[\"county_lower\"] = zip_merge_zips[\"county\"].str.lower()\n",
    "zip_merge_1 = pd.merge(zip_merge_zips, \n",
    "                       county_fips_data[[\"CountyName_low\", \"StateAbbr\", \"CountyFIPS\"]],\n",
    "                    left_on = [\"county_lower\", \"state\"],\n",
    "                    right_on = [\"CountyName_low\", \"StateAbbr\"],\n",
    "                    how=\"left\")\n",
    "print(zip_merge_1.CountyFIPS.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1.loc[pd.isnull(zip_merge_1.state_county_fips), \"state_county_fips\"] = zip_merge_1[\"CountyFIPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_merge_1[\"state_county_fips\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1 = zip_merge_1.drop(columns = [\"CountyName_low\", \"StateAbbr\", \"CountyFIPS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that most locations have been matched pretty exhaustively, we'll use just the zip code of the locations (without the state name) to fill in any info that's still missing but probably has misspecified state name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1 = pd.merge(zip_merge_1, \n",
    "                     zips[[\"county\", \"state\", \"fixed_zip\",\"longitude\", \"latitude\"]], \n",
    "                     how = \"left\", \n",
    "                      left_on = [\"respondent_zip_nn\"], \n",
    "                     right_on = [\"fixed_zip\"],\n",
    "                                suffixes = (\"\", \"_nostate\"))\n",
    "fix_cols = [\"county\", \"state\", \"fixed_zip\",\"longitude\", \"latitude\"]\n",
    "for f in fix_cols:\n",
    "    print(f)\n",
    "    print(zip_merge_1[f].isnull().sum())\n",
    "    zip_merge_1.loc[pd.isnull(zip_merge_1.latitude), f] = zip_merge_1[f+\"_nostate\"]\n",
    "    print(zip_merge_1[f].isnull().sum())\n",
    "zip_merge_1 = zip_merge_1.drop(columns = [f+\"_nostate\" for f in fix_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip_merge_1.state_county_fips.isnull().sum())\n",
    "zip_merge_1[\"county\"] = zip_merge_1[\"county\"].str.replace(\" County\", \"\")\n",
    "zip_merge_1[\"county_lower\"] = zip_merge_1[\"county\"].str.lower()\n",
    "zip_merge_1 = pd.merge(zip_merge_1, \n",
    "                       county_fips_data[[\"CountyName_low\", \"StateAbbr\", \"CountyFIPS\"]],\n",
    "                    left_on = [\"county_lower\", \"state\"],\n",
    "                    right_on = [\"CountyName_low\", \"StateAbbr\"],\n",
    "                    how=\"left\")\n",
    "print(zip_merge_1.CountyFIPS.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1.loc[pd.isnull(zip_merge_1.state_county_fips), \"state_county_fips\"] = zip_merge_1[\"CountyFIPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip_merge_1.state_county_fips.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use place lat/long to get county FIP code\n",
    "\n",
    "Get the county FIPS codes for each place using the FCC API if the latitude/longitude of the place is available but failed to match with a county in the county FIP crosswalk dataset. You'll need a Census API key to query the FCC API (saved in a `tokens.py` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcc_api_wrapper(lat, long):\n",
    "    fcc_req = (f'https://geo.fcc.gov/api/census/block/find?latitude={lat}&longitude={long}&showall'+\n",
    "                                      f'=false&format=json&key={tokens.CENSUS_KEY}')\n",
    "    resp = requests.get(fcc_req)\n",
    "    try:\n",
    "        result = resp.json()\n",
    "    except:\n",
    "        print('error in fip retrieval: ', resp)\n",
    "        print(type(resp))\n",
    "        return  np.nan, np.nan\n",
    "    try:\n",
    "        state_county_fips = result['County']['FIPS']\n",
    "        county = result['County']['name']\n",
    "\n",
    "    except:\n",
    "        print('error, could not retrieve fips for: ', lat, long)\n",
    "        return None, None\n",
    "    return state_county_fips, county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1[\"lat_long\"] = zip_merge_1[\"latitude\"].astype(str)+\"_\"+zip_merge_1[\"longitude\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(zip_merge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1[\"state_county_fips\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in zip_merge_1.index:\n",
    "    if pd.isnull(zip_merge_1.loc[i, \"latitude\"]):\n",
    "        continue\n",
    "    elif pd.isnull(zip_merge_1.loc[i, \"state_county_fips\"])==False:\n",
    "        continue\n",
    "    else:\n",
    "        fips, county_name = fcc_api_wrapper(zip_merge_1.loc[i, \"latitude\"], zip_merge_1.loc[i, \"longitude\"])\n",
    "        zip_merge_1.loc[i, \"state_county_fips\"] = fips\n",
    "        zip_merge_1.loc[i, \"fcc_county_name\"] = county_name\n",
    "    if i % 500 == 0:\n",
    "        print(\"time: \", time.time()-start)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_merge_1[\"state_county_fips\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_merge_1[\"latitude\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the places for which a FIP code could not be identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_final = zip_merge_1[pd.isnull(zip_merge_1[\"state_county_fips\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(box_path+\"failed_geocode.xlsx\", engine = 'xlsxwriter')\n",
    "fail_final.to_excel(writer, encoding = \"utf-8-sig\", index = False)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Manually fix failed locations\n",
    "\n",
    "To get the county name of the failed locations (saved as `failed_geocode.xlsx`), we searched for the place name to get the associated county and then referenced the county FIP crosswalk spreadsheet we've been using thus far. We saved the version of the dataset with the retrieved county names as `manual_countynames.xlsx` to avoid overwriting the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_counties = pd.read_excel(box_path+\"manual_countynames.xlsx\", engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_counties[\"respondent_county_low\"] = manual_counties.respondent_county_nn.str.lower()\n",
    "manual_counties = pd.merge(manual_counties,\n",
    "                          county_fips_data[[\"CountyName_low\", \"StateName\",\"CountyFIPS\"]],\n",
    "                          left_on = [\"respondent_county_low\", \"respondent_state_nn\"],\n",
    "                          right_on = [\"CountyName_low\", \"StateName\"],\n",
    "                          how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_counties = manual_counties.dropna(subset = [\"CountyFIPS\"])\n",
    "manual_counties = manual_counties[[\"charge_unique_numbers\", \"CountyFIPS\"]]\n",
    "manual_counties.columns = [\"charge_unique_numbers\", \"state_county_fips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_dict = dict(zip(manual_counties[\"charge_unique_numbers\"].to_list(),\n",
    "                      manual_counties[\"state_county_fips\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1[\"CountyFips_man\"] = zip_merge_1[\"charge_unique_numbers\"].map(manual_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1.state_county_fips.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1.loc[pd.isnull(zip_merge_1.state_county_fips), \"state_county_fips\"] = zip_merge_1[\"CountyFips_man\"]\n",
    "zip_merge_1.state_county_fips.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: Create an easy-to-merge dataset to assign county FIP codes to cases\n",
    "\n",
    "This step \"explodes\" the list of unique charge numbers identifying each case so that the geocoded locations can be perfectly matched with the charges dataset. It will result in a much longer dataset than the original geocoded places dataset because it essentially duplicates any locations with more than one charge within it for easier merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_merge_1[\"charge_nums\"] = zip_merge_1.charge_unique_numbers.str.split('|')\n",
    "explode_geocode = zip_merge_1.explode(\"charge_nums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(explode_geocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_cw = explode_geocode[[\"charge_nums\",\"city_state\",\"respondent_zip_nn\", \"state_county_fips\", \"county\",\"respondent_county_nn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_cw.charge_nums = explode_cw.charge_nums.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_cw.to_csv(box_path+\"geocoded_places.csv\",\n",
    "                 encoding = \"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
